{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GQMlmw2hwbn"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j3LQ1Qza-vV",
        "outputId": "b2c3bfff-61ed-4e9b-8c69-d66579a9ee13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.optim import Adam, SGD, RMSprop\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWNxcCtMNwJM"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InCfHKZme7ga",
        "outputId": "16abc375-de8e-4fee-d0d4-394f4d87bb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees = 10),\n",
        "    transforms.ColorJitter( brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "val_size = 5000\n",
        "train_size = len(train_dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sQRknxjoFAAF"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "lr = 1e-4\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # multi-class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgvhG8NNWih1",
        "outputId": "0a1936ab-2f4c-4d50-d007-760349f14a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cifar10Classifier(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (inception1): Inception(\n",
            "    (branch1): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (residual): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (inception2): Inception(\n",
            "    (branch1): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (residual): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (inception3): Inception(\n",
            "    (branch1): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (residual): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (inception4): Inception(\n",
            "    (branch1): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (residual): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=16384, out_features=1024, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Inception, self).__init__()\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.out_channels = out_channels * 4\n",
        "\n",
        "        if in_channels != self.out_channels:\n",
        "            self.residual = nn.Conv2d(in_channels, self.out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.residual = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1_out = self.branch1(x)\n",
        "        branch2_out = self.branch2(x)\n",
        "        branch3_out = self.branch3(x)\n",
        "        branch4_out = self.branch4(x)\n",
        "\n",
        "        inception_out = torch.cat([branch1_out, branch2_out, branch3_out, branch4_out], 1)\n",
        "        residual_out = self.residual(x)\n",
        "\n",
        "        return F.relu(inception_out + residual_out)\n",
        "class Cifar10Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Cifar10Classifier, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.inception1 = Inception(64, 64)\n",
        "        self.inception2 = Inception(256, 128)\n",
        "        self.inception3 = Inception(512, 256)\n",
        "        self.inception4 = Inception(1024, 512)\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(2048, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(1024 * 4 * 4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = self.inception3(x)\n",
        "        x = self.inception4(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model10 = Cifar10Classifier().to(device)\n",
        "print(model10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd1H-aMW68k3",
        "outputId": "cc864186-2320-4efb-c932-2f9b26da7a3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model10.load_state_dict(torch.load(\"/content/drive/MyDrive/saved_models/model16.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q21SFUcnWbU2",
        "outputId": "0379e9ec-2f6a-4f0c-81a7-f1bcc575909c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, train_loss=0.17680582208103604, val_loss=0.135933628821373. labelled 4761/5000 correctly (95.22% accuracy)\n",
            "Epoch: 1, train_loss=0.16765926986005572, val_loss=0.13776738399267197. labelled 4758/5000 correctly (95.16% accuracy)\n",
            "Epoch: 2, train_loss=0.1524897727502717, val_loss=0.16279858412742615. labelled 4715/5000 correctly (94.3% accuracy)\n",
            "Epoch: 3, train_loss=0.14581695112834375, val_loss=0.16887922039031983. labelled 4689/5000 correctly (93.78% accuracy)\n",
            "Epoch: 4, train_loss=0.14258659785456126, val_loss=0.18064341720342636. labelled 4675/5000 correctly (93.5% accuracy)\n",
            "Epoch: 5, train_loss=0.13846262357234954, val_loss=0.2100191987991333. labelled 4661/5000 correctly (93.22% accuracy)\n",
            "Epoch: 6, train_loss=0.12766707763448618, val_loss=0.175384697920084. labelled 4719/5000 correctly (94.38% accuracy)\n",
            "Epoch: 7, train_loss=0.11916795507205857, val_loss=0.23303988707065582. labelled 4645/5000 correctly (92.9% accuracy)\n",
            "Epoch: 8, train_loss=0.11899313571386867, val_loss=0.1893637627363205. labelled 4692/5000 correctly (93.84% accuracy)\n",
            "Epoch: 9, train_loss=0.11047439194917678, val_loss=0.19735424711704255. labelled 4673/5000 correctly (93.46% accuracy)\n",
            "Epoch: 10, train_loss=0.10647368193599913, val_loss=0.22039230675697327. labelled 4664/5000 correctly (93.28% accuracy)\n",
            "Epoch: 11, train_loss=0.11557437438004546, val_loss=0.2355880711853504. labelled 4641/5000 correctly (92.82000000000001% accuracy)\n",
            "Epoch: 12, train_loss=0.1001036120891571, val_loss=0.23048471666574477. labelled 4630/5000 correctly (92.60000000000001% accuracy)\n",
            "Epoch: 13, train_loss=0.055690489268716836, val_loss=0.16000754203796386. labelled 4741/5000 correctly (94.82000000000001% accuracy)\n",
            "Epoch: 14, train_loss=0.037702977916639714, val_loss=0.14128670907020568. labelled 4777/5000 correctly (95.54% accuracy)\n",
            "Epoch: 15, train_loss=0.034814546014077574, val_loss=0.15903747279644012. labelled 4757/5000 correctly (95.14% accuracy)\n",
            "Epoch: 16, train_loss=0.031176339817957744, val_loss=0.1432702155649662. labelled 4774/5000 correctly (95.48% accuracy)\n",
            "Epoch: 17, train_loss=0.028538740903304684, val_loss=0.15378850004673003. labelled 4767/5000 correctly (95.34% accuracy)\n",
            "Epoch: 18, train_loss=0.025677281416197205, val_loss=0.14832663321495057. labelled 4778/5000 correctly (95.56% accuracy)\n",
            "Epoch: 19, train_loss=0.024162695265188813, val_loss=0.13897902406863868. labelled 4786/5000 correctly (95.72% accuracy)\n",
            "Epoch: 20, train_loss=0.022857224166952073, val_loss=0.15457190581858157. labelled 4773/5000 correctly (95.46% accuracy)\n"
          ]
        }
      ],
      "source": [
        "# %%time\n",
        "optimizers = {\n",
        "    'Adam': Adam,\n",
        "    'RMSprop': RMSprop\n",
        "}\n",
        "lr = 1e-4\n",
        "results = {}\n",
        "num_epochs = 50\n",
        "criterion = nn.CrossEntropyLoss()  # multi-class\n",
        "\n",
        "for opt_name, opt_func in optimizers.items():\n",
        "  torch.save(model10.state_dict(),\"/content/drive/MyDrive/saved_models/model16_changedOptimizer.pth\")\n",
        "  optimizer = opt_func (model10.parameters(), lr=lr)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, threshold_mode='abs')\n",
        "  for epoch_no in range(num_epochs):\n",
        "\n",
        "    model10.train() \n",
        "\n",
        "    epoch_weighted_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "\n",
        "      batch_X = batch_X.to(device)\n",
        "      batch_y = batch_y.to(device)\n",
        "\n",
        "      batch_y_probs = model10(batch_X) \n",
        "\n",
        "      loss = criterion(batch_y_probs, batch_y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_weighted_loss += (len(batch_y)*loss.item())\n",
        "\n",
        "    epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n",
        "    train_losses.append(epoch_loss)\n",
        "    scheduler.step(loss)\n",
        "    torch.save(model10.state_dict(),\"/content/drive/MyDrive/saved_models/model16.pth\")\n",
        "    # validation time\n",
        "\n",
        "    model10.eval()  \n",
        "    correctly_labelled = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      val_epoch_weighted_loss = 0\n",
        "\n",
        "      for val_batch_X, val_batch_y in val_loader:\n",
        "\n",
        "        val_batch_X = val_batch_X.to(device)\n",
        "        val_batch_y = val_batch_y.to(device)\n",
        "\n",
        "        val_batch_y_probs = model10(val_batch_X)\n",
        "\n",
        "        loss = criterion(val_batch_y_probs, val_batch_y)\n",
        "        val_epoch_weighted_loss += (len(val_batch_y)*loss.item())\n",
        "\n",
        "        val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  \n",
        "\n",
        "        correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item() \n",
        "\n",
        "    val_epoch_loss = val_epoch_weighted_loss/len(val_loader.dataset)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "\n",
        "    print(f'Epoch: {epoch_no}, train_loss={epoch_loss}, val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(val_loader.dataset)} correctly ({correctly_labelled/len(val_loader.dataset)*100}% accuracy)')\n",
        "\n",
        "  print(f'Training complete on device {device}.')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
